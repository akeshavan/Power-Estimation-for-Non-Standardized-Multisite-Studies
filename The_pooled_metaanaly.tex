The pooled  or meta-analysis of regional brain volumes derived from T1-weighted MRI data across multiple sites are reliable when data is acquired with similar acquisition parameters \cite{cannon2014,multicenter01,freesurferReliability}. The inherent scanner- and sequence-related noise of MRI volumetrics under heterogeneous acquisition parameters has prompted many groups to standardize protocols across imaging sites \cite{cannon2014,adniharmonize,ADNIReview}. However, standardization across multiple sites can be prohibitively expensive and requires a significant effort to implement and maintain. %add ADNI sentence? 
At the other end of the spectrum, multisite studies without standardization can also be successful with extremely large sample sizes. The ENIGMA consortium, for example, combined scans of over 10,000 subjects from 25 sites with varying field strengths, scanner makes, acquisition protocols, and processing pipelines, providing robust phenotypic traits despite the variability of non-standardized MRI volumetrics and the power required to run a genome wide association study (GWAS) to identify modest effect sizes \cite{thompson2014enigma}. These studies raise the following question: Is there a cost-effective middle ground between fully standardizing a set of MRI scanners, and recruiting thousands of subjects across a large number of sites? %This middle ground would be more limited in effect size, but be far more cost effective. %Such a study would not only be powered to detect moderate effect sizes at reasonably high significance, but also be cost-effective.